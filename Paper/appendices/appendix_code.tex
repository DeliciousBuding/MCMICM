% Appendix A: Key Code
% 附录A：核心代码

\section*{Appendix A: Key Code}
\addcontentsline{toc}{section}{Appendix A: Key Code}
\label{appendix:code}

This appendix presents the core implementation of our analytical framework. Full code is available in our supplementary materials.

% A.1 数据预处理
\subsection*{A.1 Data Preprocessing}

\begin{lstlisting}[language=Python, caption={Data loading and preprocessing pipeline}]
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

def load_and_preprocess(filepath):
    """Load Olympic data and apply preprocessing."""
    df = pd.read_csv(filepath)
    
    # Handle missing values via forward-fill
    df = df.fillna(method='ffill')
    
    # Standardize numerical features
    scaler = StandardScaler()
    features = df.select_dtypes(include=[np.number]).columns
    df[features] = scaler.fit_transform(df[features])
    
    return df, scaler

def apply_pca(X, n_components=10):
    """Apply PCA for dimensionality reduction."""
    pca = PCA(n_components=n_components)
    Z = pca.fit_transform(X)
    explained_var = pca.explained_variance_ratio_.sum()
    print(f"Explained variance: {explained_var:.2%}")
    return Z, pca
\end{lstlisting}

% A.2 LSTM模型
\subsection*{A.2 LSTM Model Architecture}

\begin{lstlisting}[language=Python, caption={LSTM temporal encoder}]
import torch
import torch.nn as nn

class LSTMEncoder(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers=2):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, 
                           num_layers=num_layers, 
                           batch_first=True)
        
    def forward(self, x):
        # x: (batch, seq_len, input_dim)
        _, (h_n, _) = self.lstm(x)
        return h_n[-1]  # Return last hidden state
\end{lstlisting}

% A.3 XGBoost预测
\subsection*{A.3 XGBoost with Bootstrap}

\begin{lstlisting}[language=Python, caption={Bootstrap XGBoost for uncertainty quantification}]
from xgboost import XGBRegressor
from sklearn.utils import resample

def bootstrap_xgboost(X, y, n_bootstrap=100):
    """Train XGBoost with bootstrap for CI estimation."""
    predictions = []
    
    for b in range(n_bootstrap):
        X_boot, y_boot = resample(X, y)
        model = XGBRegressor(max_depth=5, n_estimators=100)
        model.fit(X_boot, y_boot)
        predictions.append(model.predict(X))
    
    predictions = np.array(predictions)
    mean_pred = predictions.mean(axis=0)
    ci_lower = np.percentile(predictions, 2.5, axis=0)
    ci_upper = np.percentile(predictions, 97.5, axis=0)
    
    return mean_pred, ci_lower, ci_upper
\end{lstlisting}
