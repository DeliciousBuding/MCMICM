% Section 8: Model Evaluation
% 模型评估

\section{Model Evaluation}
\label{sec:evaluation}

We evaluate our framework via consistency tests, case studies, and efficiency benchmarks.

\subsection{Internal Consistency}

\begin{itemize}[itemsep=0.1em]
    \item \textbf{100\%} of point estimates lie within computed bounds $[v_i^{\min}, v_i^{\max}]$.
    \item Median week-to-week change: $|\hat{v}_{i,w} - \hat{v}_{i,w-1}| = 3.2\%$; autocorrelation $\rho = 0.71$.
\end{itemize}

\subsection{Case Study Gallery}
\label{subsec:case_studies}

\textbf{Case 1: Jerry Rice (S2, 2nd Place)---Rank Rules.}
Rice averaged 22.5 judge points (3rd among finalists), yet finished 2nd. Under our MILP model with fan ranks as decision variables, the feasibility analysis shows a fan ranking of 1st or 2nd is required to explain his placement---implying massive fan support overcame moderate technical scores.

\begin{tcolorbox}[colback=dwts-fill!30!white, colframe=dwts-aux!80!black, title=\textbf{Case 1 Insight: Celebrity Power in Early Seasons}]
Jerry Rice's case demonstrates that even in early rank-based seasons, \textbf{celebrity recognition could override dancing ability}. This pattern---famous contestants surviving despite lower scores---would later culminate in the Bobby Bones controversy.
\end{tcolorbox}

\textbf{Case 2: Bobby Bones (S27, 1st Place)---The System Vulnerability Exposed.}
Bones averaged only 22.4 judge points, substantially below Milo Manheim (27.3), Evanna Lynch (26.1), and Alexis Ren (26.5). Our LP inversion yields a \emph{wide} feasible interval: $v_{\text{Bones}} \in [0.01, 0.91]$ at Week 7 (see \figref{fig:bobby_survival}). 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/bobby_bones_survival.png}
    \caption{\textbf{The Bobby Bones Anomaly: When the System Fails to Constrain.} Feasible vote interval for Bobby Bones (S27) across competition weeks. The extraordinarily wide bounds ($[1\%, 91\%]$) reveal \textbf{severe underidentification}---our mathematical constraints cannot pin down his true support level. This is not a data error; it's a \emph{structural property} of the 50/50 system: a contestant with low but non-zero judge scores can survive with virtually any fan vote share. \textbf{Takeaway:} The current system structurally allows low-scoring contestants to win if they mobilize dedicated fans.}
    \label{fig:bobby_survival}
\end{figure}

\begin{tcolorbox}[colback=dwts-warning!10!white, colframe=dwts-warning!80!black, title=\textbf{Case 2 Insight: The ``Bobby Bones Problem'' is Structural}]
The wide interval $[1\%, 91\%]$ is not uncertainty about Bobby Bones specifically---it reveals that \textbf{the current 50/50 system cannot mathematically distinguish between ``massive fan support'' and ``moderate support with low competition.''}  Any contestant with judge scores above the elimination threshold can survive with surprisingly low actual fan support. This is why we recommend the 60/40 weighted system: it would narrow such intervals by making judge scores more decisive.
\end{tcolorbox}

\textbf{Case 3: S32--S33 Model-Data Mismatch---The Judges' Save Black Box.}
Constraint slack $S^* > 0$ at specific weeks reveals outcomes not fully explainable by our model assumptions. For S32, $S^* = 2.0$ with reversal weeks at Week 2 and 3; for S33, $S^* = 1.0$ with reversal at Week 2. 

\begin{tcolorbox}[colback=dwts-warning2!15!white, colframe=dwts-warning!80!black, title=\textbf{Case 3 Insight: Successful Detection of Rule Opacity}]
These mismatches are \emph{not} evidence of manipulation---they are \textbf{successful detections of the opacity introduced by Judges' Save}. The save mechanism allows judges to override rank-based outcomes using subjective criteria (``improvement,'' ``artistry'') that cannot be predicted from scores alone. Our system correctly identifies these episodes as ``unexplainable by published rules,'' providing advance warning for media preparation.
\end{tcolorbox}

\noindent\textbf{Possible explanations for S32--S33 mismatches:}
\begin{itemize}[itemsep=0.1em]
    \item Judges' Save decisions introduce subjectivity not captured by rank-sum logic.
    \item Bottom-two selection mechanics may involve production discretion.
    \item Multi-dance week score aggregation may differ from our assumptions.
\end{itemize}

\subsection{Comparison with Alternatives}

\begin{table}[H]
    \centering
    \caption{Model Comparison: Our LP/MILP Framework vs. Baseline Approaches}
    \label{tab:model_comparison}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Approach} & \textbf{Violation Rate} & \textbf{C-index} \\
        \midrule
        Naive uniform (assume equal votes) & 73\% & 0.50 \\
        Judge-only regression & 28\% & 0.61 \\
        \textbf{Our LP/MILP Framework} & \textbf{0\% (S1--S31)} & \textbf{0.72} \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{tcolorbox}[colback=dwts-proposed!10!white, colframe=dwts-proposed!80!black, title=\textbf{Key Result: 94\% Consistency Across 34 Seasons}]
Our Dual-Core Engine achieves \textbf{perfect consistency ($S^* = 0$) for 32 of 34 seasons}. The two exceptions (S32--S33) are correctly flagged as rule-opacity events, not model failures. No alternative approach matches this combination of coverage and interpretability.
\end{tcolorbox}

\subsection{Computational Efficiency}

Total runtime: \textbf{19.8 sec} on standard laptop (i7, 16GB RAM). Per-episode LP solve: $<0.1$ sec. This enables \textbf{real-time pre-broadcast checking}---run the consistency audit before results air.
